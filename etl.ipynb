{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ETL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook con scripts para poblar un datawarehouse en Azure SQL Database de los recorridos de ecobici.\n",
    "\n",
    "Los datos de los recorridos están almacenados en una tabla de una base de datos local PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# jdbc to interact with postgresql and azure sql database\n",
    "jars = ['/home/jovyan/ecobici_rides/sqljdbc_8.2/enu/mssql-jdbc-8.2.0.jre8.jar',\n",
    " '/home/jovyan/ecobici_rides/postgresql-42.2.12.jar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master('local[*]') \\\n",
    "            .appName('etl') \\\n",
    "            .config('spark.driver.extraClassPath', ','.join(jars)) \\\n",
    "            .config(\"spark.jars\", ','.join(jars)) \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_USERNAME = '<username>'\n",
    "PG_PASSWORD = '<password>'\n",
    "PG_JDBC_URL = f'jdbc:postgresql://localhost/ecobici?user={PG_USERNAME}&password={PG_PASSWORD}'\n",
    "\n",
    "AZ_USERNAME = '<username>'\n",
    "AZ_PSSWD = \"<password>\"\n",
    "AZ_JDBC_URL = f\"jdbc:sqlserver://datadev-stuff.database.windows.net:1433;database=dw_ecobci;user={AZ_USERNAME}@datadev-stuff;password={AZ_PSSWD};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CARGA DE DATOS A LA TABLA DIM_DATE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las diferentes fechas que existen en las columna *fecha_origen_recorrido* y *fecha_destino_recorrido*.\n",
    "\n",
    "Estos valores de fecha, se cargarán en la tabla DIM_DATE.\n",
    "\n",
    "Se obtienen las diferentes fechas para dichas columnas, estas se almacenan primero en su correspondiente dataframe, luego se unifican en uno solo. A partir de ahí, se calculan valores como el año, el mes, día del mes, del año, de la semana para esa fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for _ in ['origen', 'destino']:\n",
    "    query = f'''\n",
    "    SELECT DISTINCT(DATE(fecha_{_}_recorrido))\n",
    "    FROM rides\n",
    "    '''\n",
    "    \n",
    "    df = spark.read.format('jdbc') \\\n",
    "            .option('url', PG_JDBC_URL) \\\n",
    "            .option('driver', 'org.postgresql.Driver') \\\n",
    "            .option('query', query) \\\n",
    "            .load()\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = df_list[0].union(df_list[1]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = all_dates.select( \\\n",
    "                 F.col('date').alias('date_value'), \\\n",
    "                 F.year('date').alias('year'), F.month('date').alias('month'), \\\n",
    "                 F.dayofyear('date').alias('day_of_year'), F.dayofmonth('date').alias('day_of_month'), \\\n",
    "                 F.dayofweek('date').alias('day_of_week'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = all_dates.orderBy('date_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+-----------+------------+-----------+\n",
      "|date_value|year|month|day_of_year|day_of_month|day_of_week|\n",
      "+----------+----+-----+-----------+------------+-----------+\n",
      "|2010-12-01|2010|   12|        335|           1|          4|\n",
      "|2010-12-02|2010|   12|        336|           2|          5|\n",
      "|2010-12-03|2010|   12|        337|           3|          6|\n",
      "|2010-12-04|2010|   12|        338|           4|          7|\n",
      "|2010-12-06|2010|   12|        340|           6|          2|\n",
      "|2010-12-07|2010|   12|        341|           7|          3|\n",
      "|2010-12-09|2010|   12|        343|           9|          5|\n",
      "|2010-12-10|2010|   12|        344|          10|          6|\n",
      "|2010-12-11|2010|   12|        345|          11|          7|\n",
      "|2010-12-13|2010|   12|        347|          13|          2|\n",
      "+----------+----+-----+-----------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_dates.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se cargan los datos de este dataframe en su correspondiente tabla en la base de datos en Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates.write.format('jdbc') \\\n",
    "    .mode('append') \\\n",
    "    .option('url', PG_JDBC_URL) \\\n",
    "    .option('dbtable', 'dim_date') \\\n",
    "    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CARGA DE DATOS DE LA TABLA DIM_HOUR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considero que es de interés la hora en la que se realizó el recorrido. Las horas varían entre 0 y 23, entonces es fácil generar un dataframe con estos números y, en base a éste, cargar la correspondiente tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 24, numPartitions=1) \\\n",
    "    .select(F.col('id').alias('hour_value')) \\\n",
    "    .write.option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n",
    "    .jdbc(AZ_JDBC_URL, 'dim_hour', mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CARGA DE DATOS DE LA TABLA DIM_STATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de las estaciones de ecobici están contenidos en un archivo csv con varias columnas.\n",
    "\n",
    "Se define un esquema para determinar las columnas a utilizar y el tipo de datos asociado.\n",
    "\n",
    "Finalmente, con el dataframe creado, se cargan los datos en la tabla DIM_STATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_schema = T.StructType([\n",
    "                T.StructField('station_id', T.ShortType()),\n",
    "                T.StructField('station_name', T.StringType()),\n",
    "                T.StructField('lat', T.FloatType()),\n",
    "                T.StructField('long', T.FloatType())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read.csv('all_stations.csv', my_schema, header=True).coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.orderBy('station_id') \\\n",
    ".write.option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n",
    ".jdbc(AZ_JDBC_URL, 'dim_station', mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CARGA DE DATOS A LA TABLA DE HECHOS FACT_RIDE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trabaja sobre la base de datos local en PostgreSQL con datos de los recorridos, la cual datos los identificadores de las estaciones de inicio y origen del recorrido, la fecha y hora de inicio y fin, y la duración del recorrido.\n",
    "\n",
    "Se hacen agregaciones para tener la cantidad de recorridos y el total de minutos entre dos estaciones para una fecha y hora específica.\n",
    "\n",
    "Luego es necesario obtener los diferentes ids para asociar a las tablas de dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id_estacion_origen, id_estacion_destino,\n",
    "date(fecha_origen_recorrido),\n",
    "cast(extract(hour from fecha_origen_recorrido) as smallint) as start_time,\n",
    "cast(extract(hour from fecha_destino_recorrido) as smallint) as end_time,\n",
    "count(*) as quantity, sum(duracion_recorrido) as hours_total\n",
    "FROM rides\n",
    "GROUP BY\n",
    "id_estacion_origen, id_estacion_destino,\n",
    "date(fecha_origen_recorrido),\n",
    "cast(extract(hour from fecha_origen_recorrido) as smallint),\n",
    "cast(extract(hour from fecha_destino_recorrido) as smallint)\n",
    "\"\"\"\n",
    "t = spark.read.format('jdbc') \\\n",
    ".option('url', PG_JDBC_URL) \\\n",
    ".option('query', query) \\\n",
    ".option('fetchsize', 1000000) \\\n",
    ".option('driver', 'org.postgresql.Driver') \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+----------+----------+--------+--------+-----------+\n",
      "|id_estacion_origen|id_estacion_destino|      date|start_time|end_time|quantity|hours_total|\n",
      "+------------------+-------------------+----------+----------+--------+--------+-----------+\n",
      "|                 1|                  1|2010-12-01|         9|      10|       3|        151|\n",
      "|                 1|                  1|2010-12-01|        10|      10|       1|          2|\n",
      "|                 1|                  1|2010-12-01|        11|      12|       1|         26|\n",
      "|                 1|                  1|2010-12-01|        11|      13|       1|        126|\n",
      "|                 1|                  1|2010-12-01|        13|      14|       1|         63|\n",
      "|                 1|                  1|2010-12-01|        13|      15|       1|        121|\n",
      "|                 1|                  1|2010-12-01|        16|      18|       1|        121|\n",
      "|                 1|                  1|2010-12-01|        17|      18|       1|         46|\n",
      "|                 1|                  1|2010-12-01|        18|      19|       2|        142|\n",
      "|                 1|                  1|2010-12-01|        19|      19|       2|         49|\n",
      "+------------------+-------------------+----------+----------+--------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OBTIENENDO LOS ID DE LA TABLA DIM_HOUR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour_id: integer (nullable = true)\n",
      " |-- hour_value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_hour_table = spark.read.format('jdbc') \\\n",
    "                    .option('url', AZ_JDBC_URL) \\\n",
    "                    .option('dbtable', 'dim_hour') \\\n",
    "                    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n",
    "                    .load()\n",
    "dim_hour_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+----------+--------+-----------+-------------+-----------+\n",
      "|id_estacion_origen|id_estacion_destino|      date|quantity|hours_total|start_time_id|end_time_id|\n",
      "+------------------+-------------------+----------+--------+-----------+-------------+-----------+\n",
      "|                 1|                  1|2010-12-01|       3|        151|           10|         11|\n",
      "|                 1|                  1|2010-12-01|       1|          2|           11|         11|\n",
      "|                 1|                  1|2010-12-01|       1|         26|           12|         13|\n",
      "|                 1|                  1|2010-12-01|       1|        126|           12|         14|\n",
      "|                 1|                  1|2010-12-01|       1|         63|           14|         15|\n",
      "|                 1|                  1|2010-12-01|       1|        121|           14|         16|\n",
      "|                 1|                  1|2010-12-01|       1|        121|           17|         19|\n",
      "|                 1|                  1|2010-12-01|       1|         46|           18|         19|\n",
      "|                 1|                  1|2010-12-01|       2|        142|           19|         20|\n",
      "|                 1|                  1|2010-12-01|       2|         49|           20|         20|\n",
      "+------------------+-------------------+----------+--------+-----------+-------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for time in ['start_time', 'end_time']:\n",
    "    t = t.join(F.broadcast(dim_hour_table), t[time] == dim_hour_table.hour_value)\n",
    "    t = t.drop(time, 'hour_value')\n",
    "    t = t.withColumnRenamed('hour_id', f'{time}_id')\n",
    "t.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OBTENIENDO LOS ID DE LA TABLA DIM_DATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: short (nullable = true)\n",
      " |-- date_value: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date_table = spark.read.format('jdbc') \\\n",
    "                    .option('url', AZ_JDBC_URL) \\\n",
    "                    .option('query', 'select date_id, date_value from dim_date') \\\n",
    "                    .option('fetchsize', '3000') \\\n",
    "                    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n",
    "                    .load()\n",
    "dim_date_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+-----------+-------------+-----------+------------+\n",
      "|id_estacion_origen|id_estacion_destino|quantity|hours_total|start_time_id|end_time_id|ride_date_id|\n",
      "+------------------+-------------------+--------+-----------+-------------+-----------+------------+\n",
      "|                 1|                  1|       3|        151|           10|         11|           1|\n",
      "|                 1|                  1|       1|          2|           11|         11|           1|\n",
      "|                 1|                  1|       1|         26|           12|         13|           1|\n",
      "|                 1|                  1|       1|        126|           12|         14|           1|\n",
      "|                 1|                  1|       1|         63|           14|         15|           1|\n",
      "|                 1|                  1|       1|        121|           14|         16|           1|\n",
      "|                 1|                  1|       1|        121|           17|         19|           1|\n",
      "|                 1|                  1|       1|         46|           18|         19|           1|\n",
      "|                 1|                  1|       2|        142|           19|         20|           1|\n",
      "|                 1|                  1|       2|         49|           20|         20|           1|\n",
      "+------------------+-------------------+--------+-----------+-------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = t.join(F.broadcast(dim_date_table), t.date == dim_date_table.date_value) \\\n",
    "    .drop('date', 'date_value') \\\n",
    "    .withColumnRenamed('date_id', 'ride_date_id')\n",
    "t.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OBTENIENDO LOS ID DE LA TABLA DIM_STATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unique_id: short (nullable = true)\n",
      " |-- station_id: short (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_station_table = spark.read.format('jdbc') \\\n",
    "                    .option('url', AZ_JDBC_URL) \\\n",
    "                    .option('query', 'select unique_id, station_id from dim_station') \\\n",
    "                    .option('fetchsize', '430') \\\n",
    "                    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n",
    "                    .load()\n",
    "dim_station_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+-----------+------------+-----------------+----------------------+\n",
      "|quantity|hours_total|start_time_id|end_time_id|ride_date_id|origin_station_id|destination_station_id|\n",
      "+--------+-----------+-------------+-----------+------------+-----------------+----------------------+\n",
      "|       1|         35|           14|         15|        1871|              148|                   148|\n",
      "|       1|         66|           22|         23|        1878|              148|                   148|\n",
      "|       1|         66|           17|         18|        1879|              148|                   148|\n",
      "|       1|         46|           13|         14|        1882|              148|                   148|\n",
      "|       1|         62|           11|         12|        1883|              148|                   148|\n",
      "|       1|         65|           14|         14|        1884|              148|                   148|\n",
      "|       1|         60|           15|         15|        1885|              148|                   148|\n",
      "|       1|         21|           17|         17|        1885|              148|                   148|\n",
      "|       1|         65|           16|         16|        1872|              148|                   148|\n",
      "|       1|         74|           11|         11|        1874|              148|                   148|\n",
      "+--------+-----------+-------------+-----------+------------+-----------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for old_name, new_name in {'id_estacion_origen': 'origin_station_id', 'id_estacion_destino':'destination_station_id'}.items():\n",
    "    t = t.join(dim_station_table, t[old_name] == dim_station_table.station_id) \\\n",
    "        .drop(old_name, 'station_id') \\\n",
    "        .withColumnRenamed('unique_id', new_name)\n",
    "t.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataframe ya está listo para ser cargado a la tabla de hechos, solo queda reordenar las columnas para que coincida con el esquema de la tabla en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-----------------+----------------------+------------+-------------+-----------+\n",
      "|hours_total|quantity|origin_station_id|destination_station_id|ride_date_id|start_time_id|end_time_id|\n",
      "+-----------+--------+-----------------+----------------------+------------+-------------+-----------+\n",
      "|          0|       1|                1|                     1|          74|           15|         15|\n",
      "|          0|       1|                1|                     1|          89|           15|         15|\n",
      "|          0|       1|                1|                     1|          95|           19|         19|\n",
      "|          0|       1|                1|                     1|         146|            9|          9|\n",
      "|          0|       1|                1|                     1|         163|           14|         14|\n",
      "|          0|       1|                1|                     1|         163|           18|         18|\n",
      "|          0|       1|                1|                     1|         173|           13|         13|\n",
      "|          0|       1|                1|                     1|         190|           16|         16|\n",
      "|          0|       1|                1|                     1|         190|           19|         19|\n",
      "|          0|       1|                1|                     1|         196|           11|         11|\n",
      "+-----------+--------+-----------------+----------------------+------------+-------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['hours_total', 'quantity', 'origin_station_id', 'destination_station_id', 'ride_date_id', 'start_time_id', 'end_time_id']\n",
    "t = t.select(*cols) \\\n",
    "    .orderBy(*cols)\n",
    "t.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.write.option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n",
    ".option('batchsize', 100000) \\\n",
    ".jdbc(AZ_JDBC_URL, 'fact_ride', mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y así, el datawarehouse ha sido cargado por completo :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
